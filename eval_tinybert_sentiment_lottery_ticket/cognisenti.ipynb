{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification, Trainer,\n",
    "                          TrainingArguments, BertPreTrainedModel)\n",
    "# from simpletransformers.language_modeling import LanguageModelingModel\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060', major=8, minor=6, total_memory=12050MB, multi_processor_count=28)\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_properties(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = '/home/abdullah/Code/dl/499A/best_models/epoch_3_merged_dataset_tinybert'\n",
    "TRAIN_FILE_LOC = '/home/abdullah/Code/dl/bnlp-resources/sentiment/CogniSenti/twitter_fbPost_merged_train.tsv'\n",
    "TEST_FILE_LOC = '/home/abdullah/Code/dl/bnlp-resources/sentiment/CogniSenti/twitter_fbPost_merged_test.tsv'\n",
    "EVAL_FILE_LOC = '/home/abdullah/Code/dl/bnlp-resources/sentiment/CogniSenti/twitter_fbPost_merged_dev.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/abdullah/Code/dl/bnlp-resources/sentiment/CogniSenti/twitter_fbPost_merged_train.txt',\n",
       " 'twitter_fbPost_merged_train']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tsv_to_text(tsv_file_loc):\n",
    "    file_name = tsv_file_loc.split(\"/\")[-1].split(\".\")[0]\n",
    "    txt_name = tsv_file_loc.replace(\".tsv\", \".txt\")\n",
    "    txt_name = txt_name.replace(\"split_merged\", \"texts\")\n",
    "\n",
    "    if os.path.exists(txt_name):\n",
    "        return [txt_name, file_name]\n",
    "\n",
    "    df = pd.read_csv(tsv_file_loc, sep=\"\\t\")\n",
    "\n",
    "    for txt in df[\"text\"]:\n",
    "        with open(txt_name, \"a\", encoding=\"utf8\") as f:\n",
    "            f.writelines(txt + \"\\n\")\n",
    "    return [txt_name, file_name]\n",
    "\n",
    "\n",
    "tsv_to_text(TRAIN_FILE_LOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text class_label\n",
      "0    ফ্রাঁন্সের সেই শক্তী-ধর রাজা লুই-কেত্রোজ ( রাজ...           2\n",
      "1    ৯৯% রাই ভুল করে এবার আপনাদের পালা!!! #আইকিউ_টে...           0\n",
      "2    ঢাকা শহরের হাফ ভাগ মানুষ বাকি আড়াই ভাগ অমানুষ...           2\n",
      "3    ছোট বেলায় দেখেছি চোর কে গলায় জুতার মালা পড়ি...           2\n",
      "4    ..ভাব যদি কোথায় সে ঘাসের আশ্রয়ে চলে গেল - ভা...           0\n",
      "..                                                 ...         ...\n",
      "981  বিভিন্ন দেশের মুদ্রার নাম ➺ বাংলাদেশ – টাকা। ➺...           0\n",
      "982  @Im_acs আমি ৮ বছর ধরে প্রবাসে আছি এখনো জাই নি ...           2\n",
      "983  ১৭৫৭সন ২৩সে জুন পলাশী থেকে ১৬ ই ডিসেম্বর /১৯৭১...           0\n",
      "984  সবুজ পাহাড়ে নজরকাড়া ফুল। পানখাইয়া পাড়া, খাগড়াছ...           1\n",
      "985  ধা‌র কর‌তে কর‌তে জীবন টা ম‌নে হয় শেষ হ‌য়ে যা...           2\n",
      "\n",
      "[986 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def tsv_to_df(csv_file_loc):\n",
    "    df = pd.read_csv(csv_file_loc, sep='\\t')\n",
    "    # remove id column\n",
    "    df = df.drop(columns=['id'])\n",
    "\n",
    "    # replace neutral with 0, positive with 1, negative with 2 in class_label column\n",
    "    df.loc[df['class_label'] == 'Neutral', 'class_label'] = 0\n",
    "    df.loc[df['class_label'] == 'Positive', 'class_label'] = 1\n",
    "    df.loc[df['class_label'] == 'Negative', 'class_label'] = 2\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = tsv_to_df(TEST_FILE_LOC)\n",
    "\n",
    "texts = train_df['text'].tolist()\n",
    "print(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_calculator(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  # calculate accuracy using sklearn's function\n",
    "  f1 = f1_score(labels, preds, average='weighted')\n",
    "  return {\n",
    "      'f1': f1,\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = tsv_to_text(TRAIN_FILE_LOC)\n",
    "test_list = tsv_to_text(TEST_FILE_LOC)\n",
    "eval_list = tsv_to_text(EVAL_FILE_LOC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/abdullah/Code/dl/499A/best_models/epoch_3_merged_dataset_tinybert were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /home/abdullah/Code/dl/499A/best_models/epoch_3_merged_dataset_tinybert and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "train_df = tsv_to_df(TRAIN_FILE_LOC)\n",
    "test_df = tsv_to_df(TEST_FILE_LOC)\n",
    "eval_df = tsv_to_df(EVAL_FILE_LOC)\n",
    "\n",
    "\n",
    "class TINYDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(value[idx])\n",
    "                for key, value in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_encodings = tokenizer(train_df['text'].tolist(\n",
    "), padding=True, truncation=True, max_length=512)\n",
    "test_encodings = tokenizer(test_df['text'].tolist(\n",
    "), padding=True, truncation=True, max_length=512)\n",
    "eval_encodings = tokenizer(eval_df['text'].tolist(\n",
    "), padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = TINYDataset(train_encodings, train_df['class_label'].tolist())\n",
    "test_dataset = TINYDataset(test_encodings, test_df['class_label'].tolist())\n",
    "eval_dataset = TINYDataset(eval_encodings, eval_df['class_label'].tolist())\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=3).to('cuda')\n",
    "model.manual_seed = 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Epoch Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=f\"temp\",\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=48,\n",
    "#     per_device_eval_batch_size=96,\n",
    "#     warmup_steps=500,\n",
    "#     learning_rate=5e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     overwrite_output_dir=True,\n",
    "#     logging_dir=f\"temp/logs\",\n",
    "#     logging_steps=96,\n",
    "#     save_steps=96,\n",
    "#     load_best_model_at_end=True,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     seed=14,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset\n",
    "\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 100 Epoch with 1 epoch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "***** Running training *****\n",
      "  Num examples = 4599\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 192\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2400\n",
      " 17%|█▋        | 400/2400 [01:10<05:49,  5.72it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 68.9622, 'learning_rate': 4e-05, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 17%|█▋        | 400/2400 [01:10<05:49,  5.72it/s]Saving model checkpoint to temp/checkpoint-400\n",
      "Configuration saved in temp/checkpoint-400/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 29.214954376220703, 'eval_f1': 0.43137429667915755, 'eval_runtime': 0.3753, 'eval_samples_per_second': 2624.743, 'eval_steps_per_second': 7.994, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-400/pytorch_model.bin\n",
      " 33%|███▎      | 800/2400 [02:21<04:40,  5.70it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 44.9134, 'learning_rate': 4.210526315789474e-05, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 33%|███▎      | 800/2400 [02:22<04:40,  5.70it/s]Saving model checkpoint to temp/checkpoint-800\n",
      "Configuration saved in temp/checkpoint-800/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 21.27397346496582, 'eval_f1': 0.4151399359918267, 'eval_runtime': 0.3749, 'eval_samples_per_second': 2627.068, 'eval_steps_per_second': 8.001, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-800/pytorch_model.bin\n",
      " 50%|█████     | 1200/2400 [03:32<03:26,  5.80it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 39.6746, 'learning_rate': 3.157894736842105e-05, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 1200/2400 [03:32<03:26,  5.80it/s]Saving model checkpoint to temp/checkpoint-1200\n",
      "Configuration saved in temp/checkpoint-1200/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 19.15365982055664, 'eval_f1': 0.43932719558128625, 'eval_runtime': 0.3749, 'eval_samples_per_second': 2627.497, 'eval_steps_per_second': 8.003, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-1200/pytorch_model.bin\n",
      " 67%|██████▋   | 1600/2400 [04:43<02:17,  5.80it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 38.0885, 'learning_rate': 2.105263157894737e-05, 'epoch': 66.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 67%|██████▋   | 1600/2400 [04:43<02:17,  5.80it/s]Saving model checkpoint to temp/checkpoint-1600\n",
      "Configuration saved in temp/checkpoint-1600/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 16.503175735473633, 'eval_f1': 0.509064688004136, 'eval_runtime': 0.3648, 'eval_samples_per_second': 2700.38, 'eval_steps_per_second': 8.225, 'epoch': 66.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-1600/pytorch_model.bin\n",
      " 83%|████████▎ | 2000/2400 [05:53<01:10,  5.70it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 37.1556, 'learning_rate': 1.0526315789473684e-05, 'epoch': 83.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 83%|████████▎ | 2000/2400 [05:53<01:10,  5.70it/s]Saving model checkpoint to temp/checkpoint-2000\n",
      "Configuration saved in temp/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 18.429956436157227, 'eval_f1': 0.47646174366795, 'eval_runtime': 0.3758, 'eval_samples_per_second': 2620.914, 'eval_steps_per_second': 7.982, 'epoch': 83.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-2000/pytorch_model.bin\n",
      "100%|██████████| 2400/2400 [07:05<00:00,  5.80it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 36.7251, 'learning_rate': 0.0, 'epoch': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 2400/2400 [07:05<00:00,  5.80it/s]Saving model checkpoint to temp/checkpoint-2400\n",
      "Configuration saved in temp/checkpoint-2400/config.json\n",
      "Model weights saved in temp/checkpoint-2400/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 16.3358097076416, 'eval_f1': 0.48859930055151735, 'eval_runtime': 0.3752, 'eval_samples_per_second': 2624.931, 'eval_steps_per_second': 7.995, 'epoch': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-2400 (score: 16.3358097076416).\n",
      "100%|██████████| 2400/2400 [07:06<00:00,  5.63it/s]\n",
      "***** Running training *****\n",
      "  Num examples = 4599\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 192\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 426.2698, 'train_samples_per_second': 1078.894, 'train_steps_per_second': 5.63, 'train_loss': 44.253236083984376, 'epoch': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 400/2400 [01:09<05:50,  5.71it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 36.0846, 'learning_rate': 4e-05, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 17%|█▋        | 400/2400 [01:10<05:50,  5.71it/s]Saving model checkpoint to temp/checkpoint-400\n",
      "Configuration saved in temp/checkpoint-400/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.081541061401367, 'eval_accuracy': 0.6, 'eval_runtime': 0.378, 'eval_samples_per_second': 2605.698, 'eval_steps_per_second': 7.936, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-400/pytorch_model.bin\n",
      " 33%|███▎      | 800/2400 [02:21<04:38,  5.75it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 36.3287, 'learning_rate': 4.210526315789474e-05, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 33%|███▎      | 800/2400 [02:21<04:38,  5.75it/s]Saving model checkpoint to temp/checkpoint-800\n",
      "Configuration saved in temp/checkpoint-800/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 23.655437469482422, 'eval_accuracy': 0.6538071065989848, 'eval_runtime': 0.3748, 'eval_samples_per_second': 2627.768, 'eval_steps_per_second': 8.003, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-800/pytorch_model.bin\n",
      " 50%|█████     | 1200/2400 [03:32<03:26,  5.81it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 35.9088, 'learning_rate': 3.157894736842105e-05, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 1200/2400 [03:32<03:26,  5.81it/s]Saving model checkpoint to temp/checkpoint-1200\n",
      "Configuration saved in temp/checkpoint-1200/config.json\n",
      "Model weights saved in temp/checkpoint-1200/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 16.0231990814209, 'eval_accuracy': 0.6548223350253807, 'eval_runtime': 0.3804, 'eval_samples_per_second': 2589.396, 'eval_steps_per_second': 7.886, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1600/2400 [04:43<02:20,  5.69it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 35.1235, 'learning_rate': 2.105263157894737e-05, 'epoch': 66.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 1600/2400 [04:43<02:20,  5.69it/s]Saving model checkpoint to temp/checkpoint-1600\n",
      "Configuration saved in temp/checkpoint-1600/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 15.355647087097168, 'eval_accuracy': 0.649746192893401, 'eval_runtime': 0.3679, 'eval_samples_per_second': 2677.579, 'eval_steps_per_second': 8.155, 'epoch': 66.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-1600/pytorch_model.bin\n",
      " 83%|████████▎ | 2000/2400 [05:53<01:09,  5.79it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 34.4118, 'learning_rate': 1.0526315789473684e-05, 'epoch': 83.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 83%|████████▎ | 2000/2400 [05:54<01:09,  5.79it/s]Saving model checkpoint to temp/checkpoint-2000\n",
      "Configuration saved in temp/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 14.646716117858887, 'eval_accuracy': 0.6761421319796954, 'eval_runtime': 0.3728, 'eval_samples_per_second': 2642.288, 'eval_steps_per_second': 8.048, 'epoch': 83.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-2000/pytorch_model.bin\n",
      "100%|██████████| 2400/2400 [07:04<00:00,  5.77it/s]***** Running Evaluation *****\n",
      "  Num examples = 985\n",
      "  Batch size = 384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 34.3829, 'learning_rate': 0.0, 'epoch': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2400/2400 [07:05<00:00,  5.77it/s]Saving model checkpoint to temp/checkpoint-2400\n",
      "Configuration saved in temp/checkpoint-2400/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 15.76189136505127, 'eval_accuracy': 0.649746192893401, 'eval_runtime': 0.3642, 'eval_samples_per_second': 2704.338, 'eval_steps_per_second': 8.237, 'epoch': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in temp/checkpoint-2400/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from temp/checkpoint-400 (score: 14.081541061401367).\n",
      "100%|██████████| 2400/2400 [07:05<00:00,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 425.8822, 'train_samples_per_second': 1079.876, 'train_steps_per_second': 5.635, 'train_loss': 35.37337849934896, 'epoch': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2400, training_loss=35.37337849934896, metrics={'train_runtime': 425.8822, 'train_samples_per_second': 1079.876, 'train_steps_per_second': 5.635, 'train_loss': 35.37337849934896, 'epoch': 100.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = '/home/abdullah/Code/dl/499A/best_models/1/cogni_sentiment'\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=3).to('cuda')\n",
    "model.manual_seed = 14\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bert.encoder.layer.0' in name:\n",
    "        # print(name, param.requires_grad)\n",
    "        param.requires_grad = False\n",
    "# print(\"\\n\\n\\n\")\n",
    "# for name, param in model.named_parameters():\n",
    "#     print(name, param.requires_grad)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"temp\",\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=192,\n",
    "    per_device_eval_batch_size=384,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    overwrite_output_dir=True,\n",
    "    logging_dir=f\"temp/logs\",\n",
    "    logging_steps=400,\n",
    "    save_steps=400,\n",
    "    load_best_model_at_end=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    seed=14,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=f1_calculator,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# print(cool.metrics)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30945d60abeb4d30e097bab3ad7b6ead7c1fba264acb4ede644ab6174c0ba9cb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv_499A': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
